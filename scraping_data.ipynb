{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a28430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:16:13 - scraper - INFO - Starting to scrape item IDs (max pages: 1)\n",
      "2025-06-05 14:16:15 - scraper - INFO - Found 36 new items on page 1\n",
      "2025-06-05 14:16:15 - scraper - INFO - Reached max pages limit (1)\n",
      "2025-06-05 14:16:15 - scraper - INFO - Finished scraping. Found 36 unique items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9475914', '9484809', '8962955', '9479629', '9477675', '9459989', '9483665', '9410138', '9437839', '9514140', '9514137', '9513319', '9055483', '8977140', '9403912', '9316501', '9480119', '9442779', '9493572', '9495617', '9315607', '9478943', '9467401', '9514138', '9474846', '9350637', '9497122', '9315428', '9165427', '9304577', '9514142', '9372756', '9427264', '9512985', '9204187', '8948342']\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from config.config import BASE_URL, OUTPUT_FILE\n",
    "from logger.logger import logger\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_unique_item_ids(max_pages=1):\n",
    "    \"\"\"Fetch unique item IDs from paginated pages with proper logging.\"\"\"\n",
    "    item_ids = set()\n",
    "    page = 1\n",
    "    has_more_pages = True\n",
    "    \n",
    "    logger.info(f\"Starting to scrape item IDs (max pages: {max_pages})\")\n",
    "    \n",
    "    while has_more_pages:\n",
    "        if max_pages and page > max_pages:\n",
    "            logger.info(f\"Reached max pages limit ({max_pages})\")\n",
    "            break\n",
    "            \n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        logger.debug(f\"Fetching page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.error(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find all item links and extract IDs from hrefs\n",
    "            current_page_ids = set()\n",
    "            for a in soup.select(\"a.products-i__link\"):\n",
    "                href = a.get(\"href\")\n",
    "                if href:\n",
    "                    match = re.search(r'/autos/(\\d+)', href)\n",
    "                    if match:\n",
    "                        item_id = match.group(1)\n",
    "                        current_page_ids.add(item_id)\n",
    "            \n",
    "            # Check if we found any new items on this page\n",
    "            if not current_page_ids:\n",
    "                logger.info(f\"No items found on page {page}, stopping pagination\")\n",
    "                has_more_pages = False\n",
    "            else:\n",
    "                new_items = current_page_ids - item_ids\n",
    "                if not new_items:\n",
    "                    logger.info(f\"No new items found on page {page}, stopping pagination\")\n",
    "                    has_more_pages = False\n",
    "                else:\n",
    "                    logger.info(f\"Found {len(new_items)} new items on page {page}\")\n",
    "                    item_ids.update(current_page_ids)\n",
    "                    page += 1\n",
    "                    \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Network error fetching page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "    logger.info(f\"Finished scraping. Found {len(item_ids)} unique items\")\n",
    "    return item_ids    \n",
    "\n",
    "# Run the function and print results\n",
    "item_ids = get_unique_item_ids()\n",
    "links = list(item_ids)\n",
    "print(links)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7cbe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:16:17 - scraper - INFO - Starting to process 36 items\n",
      "2025-06-05 14:16:17 - scraper - INFO - [1/36] Processing: 9475914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:16:17 - scraper - INFO - Successfully processed item: 9475914\n",
      "2025-06-05 14:16:17 - scraper - INFO - [2/36] Processing: 9484809\n",
      "2025-06-05 14:16:18 - scraper - INFO - Successfully processed item: 9484809\n",
      "2025-06-05 14:16:18 - scraper - INFO - [3/36] Processing: 8962955\n",
      "2025-06-05 14:16:19 - scraper - INFO - Successfully processed item: 8962955\n",
      "2025-06-05 14:16:19 - scraper - INFO - [4/36] Processing: 9479629\n",
      "2025-06-05 14:16:19 - scraper - INFO - Successfully processed item: 9479629\n",
      "2025-06-05 14:16:19 - scraper - INFO - [5/36] Processing: 9477675\n",
      "2025-06-05 14:16:20 - scraper - INFO - Successfully processed item: 9477675\n",
      "2025-06-05 14:16:20 - scraper - INFO - [6/36] Processing: 9459989\n",
      "2025-06-05 14:16:20 - scraper - INFO - Successfully processed item: 9459989\n",
      "2025-06-05 14:16:20 - scraper - INFO - [7/36] Processing: 9483665\n",
      "2025-06-05 14:16:21 - scraper - INFO - Successfully processed item: 9483665\n",
      "2025-06-05 14:16:21 - scraper - INFO - [8/36] Processing: 9410138\n",
      "2025-06-05 14:16:21 - scraper - INFO - Successfully processed item: 9410138\n",
      "2025-06-05 14:16:21 - scraper - INFO - [9/36] Processing: 9437839\n",
      "2025-06-05 14:16:22 - scraper - INFO - Successfully processed item: 9437839\n",
      "2025-06-05 14:16:22 - scraper - INFO - [10/36] Processing: 9514140\n",
      "2025-06-05 14:16:23 - scraper - INFO - Successfully processed item: 9514140\n",
      "2025-06-05 14:16:23 - scraper - INFO - [11/36] Processing: 9514137\n",
      "2025-06-05 14:16:24 - scraper - INFO - Successfully processed item: 9514137\n",
      "2025-06-05 14:16:24 - scraper - INFO - [12/36] Processing: 9513319\n",
      "2025-06-05 14:16:24 - scraper - INFO - Successfully processed item: 9513319\n",
      "2025-06-05 14:16:24 - scraper - INFO - [13/36] Processing: 9055483\n",
      "2025-06-05 14:16:25 - scraper - INFO - Successfully processed item: 9055483\n",
      "2025-06-05 14:16:25 - scraper - INFO - [14/36] Processing: 8977140\n",
      "2025-06-05 14:16:25 - scraper - INFO - Successfully processed item: 8977140\n",
      "2025-06-05 14:16:25 - scraper - INFO - [15/36] Processing: 9403912\n",
      "2025-06-05 14:16:26 - scraper - INFO - Successfully processed item: 9403912\n",
      "2025-06-05 14:16:26 - scraper - INFO - [16/36] Processing: 9316501\n",
      "2025-06-05 14:16:27 - scraper - INFO - Successfully processed item: 9316501\n",
      "2025-06-05 14:16:27 - scraper - INFO - [17/36] Processing: 9480119\n",
      "2025-06-05 14:16:27 - scraper - INFO - Successfully processed item: 9480119\n",
      "2025-06-05 14:16:27 - scraper - INFO - [18/36] Processing: 9442779\n",
      "2025-06-05 14:16:28 - scraper - INFO - Successfully processed item: 9442779\n",
      "2025-06-05 14:16:28 - scraper - INFO - [19/36] Processing: 9493572\n",
      "2025-06-05 14:16:29 - scraper - INFO - Successfully processed item: 9493572\n",
      "2025-06-05 14:16:29 - scraper - INFO - [20/36] Processing: 9495617\n",
      "2025-06-05 14:16:30 - scraper - INFO - Successfully processed item: 9495617\n",
      "2025-06-05 14:16:30 - scraper - INFO - [21/36] Processing: 9315607\n",
      "2025-06-05 14:16:30 - scraper - INFO - Successfully processed item: 9315607\n",
      "2025-06-05 14:16:30 - scraper - INFO - [22/36] Processing: 9478943\n",
      "2025-06-05 14:16:31 - scraper - INFO - Successfully processed item: 9478943\n",
      "2025-06-05 14:16:31 - scraper - INFO - [23/36] Processing: 9467401\n",
      "2025-06-05 14:16:32 - scraper - INFO - Successfully processed item: 9467401\n",
      "2025-06-05 14:16:32 - scraper - INFO - [24/36] Processing: 9514138\n",
      "2025-06-05 14:16:32 - scraper - INFO - Successfully processed item: 9514138\n",
      "2025-06-05 14:16:32 - scraper - INFO - [25/36] Processing: 9474846\n",
      "2025-06-05 14:16:33 - scraper - INFO - Successfully processed item: 9474846\n",
      "2025-06-05 14:16:33 - scraper - INFO - [26/36] Processing: 9350637\n",
      "2025-06-05 14:16:33 - scraper - INFO - Successfully processed item: 9350637\n",
      "2025-06-05 14:16:33 - scraper - INFO - [27/36] Processing: 9497122\n",
      "2025-06-05 14:16:34 - scraper - INFO - Successfully processed item: 9497122\n",
      "2025-06-05 14:16:34 - scraper - INFO - [28/36] Processing: 9315428\n",
      "2025-06-05 14:16:35 - scraper - INFO - Successfully processed item: 9315428\n",
      "2025-06-05 14:16:35 - scraper - INFO - [29/36] Processing: 9165427\n",
      "2025-06-05 14:16:35 - scraper - INFO - Successfully processed item: 9165427\n",
      "2025-06-05 14:16:35 - scraper - INFO - [30/36] Processing: 9304577\n",
      "2025-06-05 14:16:36 - scraper - INFO - Successfully processed item: 9304577\n",
      "2025-06-05 14:16:36 - scraper - INFO - [31/36] Processing: 9514142\n",
      "2025-06-05 14:16:37 - scraper - INFO - Successfully processed item: 9514142\n",
      "2025-06-05 14:16:37 - scraper - INFO - [32/36] Processing: 9372756\n",
      "2025-06-05 14:16:37 - scraper - INFO - Successfully processed item: 9372756\n",
      "2025-06-05 14:16:37 - scraper - INFO - [33/36] Processing: 9427264\n",
      "2025-06-05 14:16:38 - scraper - INFO - Successfully processed item: 9427264\n",
      "2025-06-05 14:16:38 - scraper - INFO - [34/36] Processing: 9512985\n",
      "2025-06-05 14:16:39 - scraper - INFO - Successfully processed item: 9512985\n",
      "2025-06-05 14:16:39 - scraper - INFO - [35/36] Processing: 9204187\n",
      "2025-06-05 14:16:39 - scraper - INFO - Successfully processed item: 9204187\n",
      "2025-06-05 14:16:39 - scraper - INFO - [36/36] Processing: 8948342\n",
      "2025-06-05 14:16:40 - scraper - INFO - Successfully processed item: 8948342\n",
      "2025-06-05 14:16:40 - scraper - INFO - Successfully saved 36 items to data/items_data.json\n"
     ]
    }
   ],
   "source": [
    "def extract_item_data(link):\n",
    "    \"\"\"Extract detailed item data from individual product page with comprehensive logging\"\"\"\n",
    "    url = \"https://turbo.az/autos/\" + link\n",
    "    logger.debug(f\"Starting to extract data from: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Failed to fetch {url} - Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract item ID\n",
    "        item_id = link.split(\"/\")[-1]\n",
    "        logger.debug(f\"Processing item ID: {item_id}\")\n",
    "\n",
    "        # Extract amount and currency using meta tags\n",
    "        meta_tag = soup.find('meta', {'property': 'og:title'})\n",
    "\n",
    "        # Extract content attribute\n",
    "        content = meta_tag.get('content')\n",
    "\n",
    "        # Use regex to find price followed by either USD or AZN (case-insensitive)\n",
    "        match = re.search(r'(\\d[\\d\\s]*\\d)\\s+(USD|AZN)', content, re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            amount = match.group(1).replace(' ', '')  # Remove space\n",
    "            currency = match.group(2).upper()  # Get the currency type\n",
    "\n",
    "        logger.debug(f\"Extracted price: {amount} {currency}\")\n",
    "\n",
    "        # Extract property fields dynamically\n",
    "        properties = {}\n",
    "        prop_div = soup.find(\"div\", class_=\"product-properties__column\")\n",
    "        if prop_div:\n",
    "            for prop in prop_div.find_all(\"div\", class_=\"product-properties__i\"):\n",
    "                try:\n",
    "                    name = prop.find(\"label\", class_=\"product-properties__i-name\").get_text(strip=True)\n",
    "                    value = prop.find(\"span\", class_=\"product-properties__i-value\").get_text(strip=True)\n",
    "                    properties[name] = value\n",
    "                    logger.debug(f\"Found property: {name} = {value}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to extract property: {str(e)}\", exc_info=True)\n",
    "\n",
    "        # Construct result dictionary\n",
    "        item_data = {\n",
    "            'item_id': item_id,\n",
    "            'amount': amount,\n",
    "            'currency': currency,\n",
    "        }\n",
    "\n",
    "        # Add all found properties dynamically\n",
    "        item_data.update(properties)\n",
    "        \n",
    "        logger.info(f\"Successfully processed item: {item_id}\")\n",
    "        return item_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Network error while processing {url}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing {url}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_all_items():\n",
    "    \"\"\"Main scraping function that processes all items with progress tracking\"\"\"\n",
    "    if not links:\n",
    "        logger.warning(\"No links to process - empty list provided\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    logger.info(f\"Starting to process {len(links)} items\")\n",
    "\n",
    "    for idx, link in enumerate(links, 1):\n",
    "        logger.info(f\"[{idx}/{len(links)}] Processing: {link}\")\n",
    "        data = extract_item_data(link)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "            logger.debug(f\"Added item {data.get('item_id')} to results\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to process item: {link}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Successfully saved {len(results)} items to {OUTPUT_FILE}\")\n",
    "        if len(results) != len(links):\n",
    "            logger.warning(f\"Processed {len(results)} out of {len(links)} items ({(len(results)/len(links))*100:.1f}% success rate)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save data to {OUTPUT_FILE}: {str(e)}\", exc_info=True)\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
