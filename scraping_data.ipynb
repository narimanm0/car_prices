{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a28430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 20:16:27 - scraper - INFO - Starting to scrape item IDs (max pages: 2)\n",
      "2025-06-04 20:16:29 - scraper - INFO - Found 36 new items on page 1\n",
      "2025-06-04 20:16:33 - scraper - INFO - Found 35 new items on page 2\n",
      "2025-06-04 20:16:33 - scraper - INFO - Reached max pages limit (2)\n",
      "2025-06-04 20:16:33 - scraper - INFO - Finished scraping. Found 71 unique items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9485366', '9470045', '9499757', '9441472', '9512961', '9506811', '9511123', '9493725', '9512964', '8860926', '9512956', '9512959', '9468381', '9122833', '9502951', '9433877', '9504555', '9512955', '9499025', '9248630', '7796314', '9512958', '9459711', '9316223', '9448228', '9436263', '9419397', '9512965', '9457851', '9512972', '9435389', '8904645', '9473032', '9512940', '9059811', '9480968', '9201400', '9512971', '9444787', '9399711', '9004270', '9310974', '9428149', '8801689', '9486312', '8854919', '9479885', '9512952', '9512926', '9321904', '9480305', '9512954', '9512962', '9270343', '9490749', '9512905', '9400495', '9512920', '9375328', '9502894', '9512909', '9261772', '9434653', '8210854', '9474554', '9500610', '9497935', '9462931', '9072179', '9498204', '9508303']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from config.config import BASE_URL\n",
    "from logger.logger import logger\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_unique_item_ids(max_pages=2):\n",
    "    \"\"\"Fetch unique item IDs from paginated pages with proper logging.\"\"\"\n",
    "    item_ids = set()\n",
    "    page = 1\n",
    "    has_more_pages = True\n",
    "    \n",
    "    logger.info(f\"Starting to scrape item IDs (max pages: {max_pages})\")\n",
    "    \n",
    "    while has_more_pages:\n",
    "        if max_pages and page > max_pages:\n",
    "            logger.info(f\"Reached max pages limit ({max_pages})\")\n",
    "            break\n",
    "            \n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        logger.debug(f\"Fetching page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.error(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find all item links and extract IDs from hrefs\n",
    "            current_page_ids = set()\n",
    "            for a in soup.select(\"a.products-i__link\"):\n",
    "                href = a.get(\"href\")\n",
    "                if href:\n",
    "                    match = re.search(r'/autos/(\\d+)', href)\n",
    "                    if match:\n",
    "                        item_id = match.group(1)\n",
    "                        current_page_ids.add(item_id)\n",
    "            \n",
    "            # Check if we found any new items on this page\n",
    "            if not current_page_ids:\n",
    "                logger.info(f\"No items found on page {page}, stopping pagination\")\n",
    "                has_more_pages = False\n",
    "            else:\n",
    "                new_items = current_page_ids - item_ids\n",
    "                if not new_items:\n",
    "                    logger.info(f\"No new items found on page {page}, stopping pagination\")\n",
    "                    has_more_pages = False\n",
    "                else:\n",
    "                    logger.info(f\"Found {len(new_items)} new items on page {page}\")\n",
    "                    item_ids.update(current_page_ids)\n",
    "                    page += 1\n",
    "                    \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Network error fetching page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "    logger.info(f\"Finished scraping. Found {len(item_ids)} unique items\")\n",
    "    return item_ids    \n",
    "\n",
    "# Run the function and print results\n",
    "item_ids = get_unique_item_ids()\n",
    "links = list(item_ids)\n",
    "print(links)\n",
    "print(len(links))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
